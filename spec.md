# SlideNode — Functional Specification

**Version:** 1.1 (2026-02-19)

## Overview

SlideNode transforms academic/research PDFs into structured teaching slide decks. The pipeline extracts factual content, organizes it into a pedagogical outline, and exports to Markdown or PowerPoint.

## Presentation Style

**Teaching courseware** — slides are organized by knowledge topics and concepts, emphasizing:
- Clear definitions and terminology
- Formulas and methods with context
- Examples, results, and their implications
- Limitations and open questions

This is not a summary or abstract — it is a structured learning resource.

## Structure Rules

- **Sections** (3–8 per document): Top-level knowledge areas, decided by LLM based on content analysis
- **Subsections** (1–5 per section): Specific topics within each area, count decided by LLM
- **Bullets** (1–8 per subsection): Individual factual learning points, each backed by ≥1 citation
- Exactly 3 levels — no nesting deeper than bullet level

## Output Language

- Detected automatically from source document using `langdetect`
- All LLM-generated content (headings, annotations, summaries) follows source language
- Stored in `Document.language` field

## Export Formats

### Markdown (`GET /v1/documents/{id}/export.md`)
- H1 = document title
- H2 = section heading + summary note
- H3 = subsection heading + blockquote annotation
- Bullets with footnote-style citations

### PowerPoint (`GET /v1/documents/{id}/export.pptx`)
- 16:9 widescreen slides
- Slide types:
  - **Title page**: Document title + "Generated by SlideNode"
  - **Section divider**: Blue heading + gray summary note
  - **Content page**: Section label (gray) + subsection heading (black) + annotation (gray) + bullet list
- Maximum 6 bullets per slide; overflow creates continuation slides
- Clean white background, minimal styling

## LLM Provider

- **Primary**: OpenRouter (OpenAI-compatible endpoint)
- **Configuration**: `LLM_BASE_URL`, `LLM_API_KEY`, `LLM_MODEL` in `.env`
- **Testing**: `LLM_PROVIDER=mock` for offline development
- **Anthropic**: Supported via `LLM_PROVIDER=anthropic` with separate base URL and auth

## Pipeline Steps

1. **Ingest PDF** — receive upload, store in configured storage backend
2. **Parse PDF** — extract text via pypdf, split into token-sized chunks
3. **Detect Language** — langdetect on first 5 chunks, fallback to "en"
4. **Extract Facts** — LLM extracts up to 8 structured facts per chunk
5. **Fuzzy Dedupe** — thefuzz token_sort_ratio removes near-duplicates (threshold: 86%)
6. **Build Outline** — LLM organizes facts into section/subsection structure
7. **Write Annotations** — LLM generates 1-3 sentence teaching notes per subsection
8. **Align Citations** — keyword-overlap sliding window finds best quote snippets
9. **Persist & Validate** — save to DB, enforce quality gates (coverage ≥ 0.85, citation completeness = 1.0)

## Quality Gates

| Gate | Threshold | Behavior on Failure |
|------|-----------|-------------------|
| Citation completeness | 1.0 (every bullet) | `CITATION_INCOMPLETE` error |
| Coverage ratio | ≥ 0.85 | `QUALITY_GATE_FAILED` error |
| Page limit | ≤ 200 | `DOC_TOO_LARGE` error |
| LLM output validity | Valid JSON + schema | Retry up to `llm_max_retries`, then `LLM_OUTPUT_INVALID` |

## Dependencies

- `python-pptx` — PowerPoint generation
- `langdetect` — source language detection
- `thefuzz[speedup]` — fuzzy string matching for deduplication
- `pypdf` — PDF text extraction
- `httpx` — LLM API calls
